{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib #For downloading webpages\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup # For Parsing HTML Documents\n",
    "import requests\n",
    "import re\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from warnings import warn\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import numpy as np, pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Create lists to save data--------------------------------------------\n",
    "title = list()\n",
    "year = list()\n",
    "certi = list()\n",
    "time = list()\n",
    "release_date = list()\n",
    "rating = list()\n",
    "metascore = list()\n",
    "budget = list()\n",
    "Opening_Weekend_USA = list()\n",
    "Gross_USA = list()\n",
    "Cumulative_Worldwide_Gross = list()\n",
    "\n",
    "# Read list of movies to be scraped\n",
    "movie_list = pd.read_csv(\"C:/Users/neilr/OneDrive/Desktop/Notes/Matt 891/Matt_imdb project/IMDB_list.csv\")\n",
    "url_list = movie_list['movie_imdb_link'].to_list()\n",
    "\n",
    "#Itterate through each movie to scrape data\n",
    "for url in url_list:\n",
    "    #Read The url---------------------------------------------------------\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    if page.status_code != 200:\n",
    "        continue\n",
    "        \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    #Get Basic Data-------------------------------------------------------\n",
    "    Basicdata = list()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Get the whole basic data and split it in different attributes.\n",
    "        t = soup.h1.text\n",
    "        y = soup.h1.span.a.text\n",
    "        a = soup.find('div', class_=\"subtext\").text\n",
    "        q = a.replace(\"   \",\"\")\n",
    "        q = q.replace(\"  \",\"\")\n",
    "        q = q.replace(\",\",\"\")\n",
    "        li = q.split('\\n')\n",
    "\n",
    "        Basicdata.extend([t,y,li[1],li[3],li[-2]])\n",
    "\n",
    "        title.append(Basicdata[0])\n",
    "        year.append(Basicdata[1])\n",
    "        certi.append(Basicdata[2])\n",
    "        time.append(Basicdata[3])\n",
    "        release_date.append(Basicdata[4])\n",
    "    except:\n",
    "        title.append(\"-\")\n",
    "        year.append(\"-\")\n",
    "        certi.append(\"-\")\n",
    "        time.append(\"-\")\n",
    "        release_date.append(\"-\")\n",
    "        \n",
    "    #Get Rating and MetaScore----------------------------------------------\n",
    "    try:\n",
    "        r = soup.find('div', class_=\"ratingValue\").span.text\n",
    "        rating.append(r)\n",
    "    except:\n",
    "        rating.append(\"-\")\n",
    "        \n",
    "    flag = 0\n",
    "    try:\n",
    "        m = soup.find('div', class_=\"metacriticScore score_favorable titleReviewBarSubItem\").span.text\n",
    "        metascore.append(m)\n",
    "        flag = 1\n",
    "    except:    \n",
    "        try:\n",
    "            m = soup.find('div', class_=\"metacriticScore score_mixed titleReviewBarSubItem\").span.text\n",
    "            metascore.append(m)\n",
    "        except:\n",
    "            metascore.append(\"-\")\n",
    "\n",
    "    #Get the financials----------------------------------------------------\n",
    "    count = [0,0,0,0]\n",
    "    for h4 in soup.find_all('h4'):\n",
    "        try:\n",
    "            if \"Budget:\" in h4:\n",
    "                count[0] = count[0] + 1\n",
    "                budget.append(h4.next_sibling.strip())\n",
    "        except:\n",
    "            budget.append(\"-\")\n",
    "            \n",
    "        try:\n",
    "            if \"Opening Weekend USA:\" in h4:\n",
    "                count[1] = count[1] + 1\n",
    "                Opening_Weekend_USA.append(h4.next_sibling.strip())\n",
    "        except:\n",
    "            Opening_Weekend_USA.append(\"-\")\n",
    "\n",
    "        try:\n",
    "            if \"Gross USA:\" in h4:\n",
    "                count[2] = count[2] + 1\n",
    "                Gross_USA.append(h4.next_sibling.strip())\n",
    "        except:\n",
    "            Gross_USA.append(\"-\")\n",
    "            \n",
    "        try:\n",
    "            if \"Cumulative Worldwide Gross:\" in h4:\n",
    "                count[3] = count[3] + 1\n",
    "                Cumulative_Worldwide_Gross.append(h4.next_sibling.strip())\n",
    "        except:\n",
    "            Cumulative_Worldwide_Gross.append(\"-\")\n",
    "            \n",
    "    if count[0] == 0:\n",
    "        budget.append(\"-\")\n",
    "    if count[1] == 0:\n",
    "        Opening_Weekend_USA.append(\"-\")\n",
    "    if count[2] == 0:\n",
    "        Gross_USA.append(\"-\")\n",
    "    if count[3] == 0:\n",
    "        Cumulative_Worldwide_Gross.append(\"-\")\n",
    "\n",
    "# Store the whole data in a DataFrame\n",
    "df = pd.DataFrame(list(zip(title,year,certi,time,release_date,rating,metascore,budget,Opening_Weekend_USA,\n",
    "                       Gross_USA,Cumulative_Worldwide_Gross)), columns =['title','year','certi','time','release_date',\n",
    "                                                                         'rating','metascore','budget','Opening_Weekend_USA',\n",
    "                       'Gross_USA','Cumulative_Worldwide_Gross'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wall time: 35min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\neilr\\OneDrive\\Desktop\\Notes\\Matt 891\\Matt_imdb project\\Web_Scrape_data.csv',index = False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------END---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
