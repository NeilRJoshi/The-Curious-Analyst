{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Web Crawler - fetch  links from wiki page\n",
    "\n",
    "Write a web crawler in a Python notebook.  The crawler should be given a single URL entry point to a webpage with links to a  number of static web pages in the same domain.  From that URL entry point, the crawler should gather additional “target” URLs to webpages from inside the same domain as the entry URL and visit those web pages to collect the page titles for each (“page_title_target”).  For each page aside from the entry URL entry, I want you to collect:\n",
    "    The source URL (url_sorce) of an HTML webpage that you gathered the target URL from\n",
    "    The target URL (url_target) to an HTML webpage you discovered through crawling\n",
    "    The title  (page_title_target) of the web page rendered by the (url_target) URL\n",
    "\n",
    "From the entry webpage you will gather an additional 99 unique target URLs to other webpages, using them as additional source_urls as needed to obtain the additional target_urls.  Do note that we are collecting html webpage URLs and not URLs other resources like images; and these target URLs should be unique (i.e. not pointing to a different section of the same page).\n",
    "\n",
    "Write the collected url_sorce, url_target, and page_title_target data into a CSV file as described in the deliverables section.  The initial entry point into the crawl should be given a “None” value for its url_source.  Including the entry point, you should have 100 target_urls by the end of the crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Read the URL HTML Page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib #For downloading webpages\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup # For Parsing HTML Documents\n",
    "\n",
    "input_url = 'https://en.wikipedia.org/wiki/2019_in_film'\n",
    "with urllib.request.urlopen(input_url) as response:\n",
    "    Read1 = response.read() #Reads the whole HTML Document\n",
    "\n",
    "Read1_BS = BeautifulSoup(Read1)\n",
    "#print(Read1_BS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'> div\n",
      "<class 'bs4.element.Tag'> div\n",
      "<class 'bs4.element.Tag'> div\n",
      "<class 'bs4.element.Tag'> div\n",
      "<class 'bs4.element.Tag'> div\n",
      "<class 'bs4.element.Tag'> div\n",
      "<class 'bs4.element.Tag'> script\n",
      "<class 'bs4.element.Tag'> script\n",
      "<class 'bs4.element.Tag'> script\n"
     ]
    }
   ],
   "source": [
    "# to identify childs in body\n",
    "for child in Read1_BS.body:\n",
    "    if type(child) == bs4.element.Tag:\n",
    "        print(type(child), child.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Filter out the a-Tags\n",
    "\n",
    "#### then geting unique href values in a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = Read1_BS.body.find_all('a') # This will give us all a-tags (a tags contain href's)\n",
    "#a_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1272"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "href_found = set() #Using set to get unique values\n",
    "for l in a_tags:\n",
    "    if 'href' in l.attrs:\n",
    "        href_found.add(l.attrs['href'])   # (.attrs) reads the attributes if any. href is an attribute of 'a' Tag\n",
    "\n",
    "len(href_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_found # here we can observe that the links obtained are not in default URL format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Cleaning the extracted URL's\n",
    "\n",
    "### We will first analyse the source URL in which we will get the netloc and then convert the extracted links in the default url format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='https', netloc='en.wikipedia.org', path='/wiki/2019_in_film', params='', query='', fragment='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_source_url = urllib.parse.urlparse(input_url)  # Analyse the input url\n",
    "check_source_url  # check the value of netloc in output. we will be using it in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_discovered_links = set()\n",
    "# Clean up discovered links to remove fragments and URLs\n",
    "# from different \"netloc\" sources\n",
    "for link in href_found:\n",
    "    parsed_link = urllib.parse.urlparse(link) # This will give the path. note that here netloc will be blank\n",
    "    #print(parsed_link)\n",
    "    # Try to join relative links to base URL\n",
    "    # Trying to normalize our links to be absolute\n",
    "    joined_link = urllib.parse.urljoin(input_url, link)\n",
    "    parsed_joined_link = urllib.parse.urlparse(joined_link)\n",
    "    if check_source_url.netloc == parsed_joined_link.netloc:\n",
    "        cleaned_discovered_links.add(joined_link)\n",
    "\n",
    "#cleaned_discovered_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_discovered_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the fragment part of the links if any to get unique links\n",
    "\n",
    "unique_links  = set()\n",
    "for links in cleaned_discovered_links:  # As the links are of datatype str I am using the split function to get unique links\n",
    "    links = links.split('#')[0]\n",
    "    links = links.split('?')[0]\n",
    "\n",
    "    unique_links.add(links)\n",
    "\n",
    "#unique_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Checking if Links are authentic and fetching first 100 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1078"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This step takes a lot of time to execute. Ask Matt if any method to speed up this process\n",
    "\n",
    "webpage_only = set()\n",
    "for check in unique_links:\n",
    "        #print(check)\n",
    "        \n",
    "        try:\n",
    "            with urllib.request.urlopen\\\n",
    "            (check, data=None, timeout=5) as response:\n",
    "                if 'text/html' in response.headers['Content-Type']: # Checks the contect type which is under network - headers\n",
    "\n",
    "                    webpage_only.add(check)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "len(webpage_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we will fetch 100 links as required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_links_list = list(webpage_only)\n",
    "url_target = unique_links_list[0:100] #Slicing\n",
    "url_target[0] = 'https://en.wikipedia.org/wiki/2019_in_film'\n",
    "#url_target  # List of 100 unique links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Fectching titles of 100 URL Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "page_title_target = list()\n",
    "url_source = list()\n",
    "import urllib.request\n",
    "\n",
    "for t in url_target:\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(t))\n",
    "    page_title_target.append(soup.title.string) # This will give Page titles\n",
    "    url_source.append('https://en.wikipedia.org/wiki/2019_in_film') # To create source URL Data\n",
    "\n",
    "\n",
    "#print(page_title_target)\n",
    "#print(len(page_title_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_source[0] = None # As required, First data in url_source must be None\n",
    "#print(url_source)\n",
    "#print(len(url_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final step - Export into csv in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# Adding double quotes to all elements as required\n",
    "A = list()\n",
    "for i in url_source:\n",
    "    j = '\"' + str(i) + '\"'\n",
    "    A.append(j)\n",
    "url_source = A\n",
    "#print(url_source)\n",
    "\n",
    "\n",
    "A = list()\n",
    "for i in url_target:\n",
    "    j = '\"' + str(i) + '\"'\n",
    "    A.append(j)\n",
    "url_target = A\n",
    "#print(url_target)\n",
    "\n",
    "A = list()\n",
    "for i in page_title_target:\n",
    "    j = '\"' + str(i) + '\"'\n",
    "    A.append(j)\n",
    "page_title_target = A\n",
    "#print(page_title_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_source</th>\n",
       "      <th>url_target</th>\n",
       "      <th>page_title_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\"None\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"2019 in film - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/List_of_Malayal...</td>\n",
       "      <td>\"List of Malayalam films of 2019 - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Giulio_Brogi\"</td>\n",
       "      <td>\"Giulio Brogi - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/(I%27m_Gonna)_L...</td>\n",
       "      <td>\"(I'm Gonna) Love Me Again - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/1930s_in_film\"</td>\n",
       "      <td>\"1930s in film - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/The_Boat_That_R...</td>\n",
       "      <td>\"The Boat That Rocked - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/1998_in_film\"</td>\n",
       "      <td>\"1998 in film - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/List_of_highest...</td>\n",
       "      <td>\"List of highest-grossing films in Japan - Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/Quentin_Tarantino\"</td>\n",
       "      <td>\"Quentin Tarantino - Wikipedia\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/2019_in_film\"</td>\n",
       "      <td>\"https://en.wikipedia.org/wiki/The_Death_of_Ma...</td>\n",
       "      <td>\"The Death of Mario Ricci - Wikipedia\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      url_source  \\\n",
       "0                                         \"None\"   \n",
       "1   \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "2   \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "3   \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "4   \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "..                                           ...   \n",
       "95  \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "96  \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "97  \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "98  \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "99  \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "\n",
       "                                           url_target  \\\n",
       "0        \"https://en.wikipedia.org/wiki/2019_in_film\"   \n",
       "1   \"https://en.wikipedia.org/wiki/List_of_Malayal...   \n",
       "2        \"https://en.wikipedia.org/wiki/Giulio_Brogi\"   \n",
       "3   \"https://en.wikipedia.org/wiki/(I%27m_Gonna)_L...   \n",
       "4       \"https://en.wikipedia.org/wiki/1930s_in_film\"   \n",
       "..                                                ...   \n",
       "95  \"https://en.wikipedia.org/wiki/The_Boat_That_R...   \n",
       "96       \"https://en.wikipedia.org/wiki/1998_in_film\"   \n",
       "97  \"https://en.wikipedia.org/wiki/List_of_highest...   \n",
       "98  \"https://en.wikipedia.org/wiki/Quentin_Tarantino\"   \n",
       "99  \"https://en.wikipedia.org/wiki/The_Death_of_Ma...   \n",
       "\n",
       "                                    page_title_target  \n",
       "0                          \"2019 in film - Wikipedia\"  \n",
       "1       \"List of Malayalam films of 2019 - Wikipedia\"  \n",
       "2                          \"Giulio Brogi - Wikipedia\"  \n",
       "3             \"(I'm Gonna) Love Me Again - Wikipedia\"  \n",
       "4                         \"1930s in film - Wikipedia\"  \n",
       "..                                                ...  \n",
       "95                 \"The Boat That Rocked - Wikipedia\"  \n",
       "96                         \"1998 in film - Wikipedia\"  \n",
       "97  \"List of highest-grossing films in Japan - Wik...  \n",
       "98                    \"Quentin Tarantino - Wikipedia\"  \n",
       "99             \"The Death of Mario Ricci - Wikipedia\"  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use pandas for this job.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#data = [url_source, url_target, page_title_target]\n",
    "crawl = pd.DataFrame()\n",
    "\n",
    "crawl['url_source'] = url_source\n",
    "crawl['url_target'] = url_target\n",
    "crawl['page_title_target'] = page_title_target\n",
    "crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Export dataframe to csv\n",
    "crawl.to_csv (r'C:\\Users\\neilr\\OneDrive\\Desktop\\Notes\\Matt 891\\crawl.csv', index = None, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
